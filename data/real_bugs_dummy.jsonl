{"bug_type": "varmisuse", "project": "Duffy", "project_url": "https://github.com/hbridge/Duffy", "commit_sha": "3b55cba21976e68dc0de2b4f050048e53ee51d29", "comodified": false, "input_text": "def processBatch(photosToProcess):\n\ttotal = 0\n\tstrandPhotosToCreate = list()\n\tstrandUsersToCreate = list()\n\n\ttimeWithinSecondsForNotification = 10 # seconds\n\n\ttimeTakendelta = datetime.timedelta(hours=3)\t\n\n\t# make a list of users whose photos are here\n\tuserIdList = list()\n\ta = datetime.datetime.now()\n\t\n\tfor photo in photosToProcess:\n\t\tif photo.time_taken > datetime.datetime.utcnow().replace(tzinfo=pytz.utc)-timeTakendelta:\n\t\t\tuserIdList.append(photo.user_id)\n\n\tif len(userIdList) > 0:\n\t\tprocessUserIdsForFriendGPSInfoAppTask.delay(userIdList)\n\n\t# Group photos by users, then iterate through all users one at a time, fetching the cache as we go\n\tphotosByUser = dict()\n\tfor photo in photosToProcess:\n\t\tif photo.user not in photosByUser:\n\t\t\tphotosByUser[photo.user] = list()\n\t\tphotosByUser[photo.user].append(photo)\n\n\n\tfor user, photos in photosByUser.iteritems():\n\t\tlogger.debug(\"Starting a run with %s photos, took %s milli\" % (len(photos), ((datetime.datetime.now()-a).microseconds/1000) + (datetime.datetime.now()-a).seconds*1000))\n\t\tb = datetime.datetime.now()\n\t\tstrandsCreated = list()\n\t\tstrandsAddedTo = list()\n\t\tstrandsDeleted = 0\n\n\t\tphotosByStrandId = dict()\n\t\tusersByStrandId = dict()\n\t\tallUsers = list()\n\n\t\t# Used for notifications\n\t\tphotoToStrandIdDict = dict()\n\t\tphotos = list(photos)\n\n\t\ttimeHigh = photos[0].time_taken + datetime.timedelta(minutes=constants.TIME_WITHIN_MINUTES_FOR_NEIGHBORING)\n\t\ttimeLow = photos[-1].time_taken - datetime.timedelta(minutes=constants.TIME_WITHIN_MINUTES_FOR_NEIGHBORING)\n\n\t\tstrandsCache = list(Strand.objects.prefetch_related('users', 'photos').filter(user=user).filter(private=1).filter((Q(first_photo_time__gt=timeLow) & Q(first_photo_time__lt=timeHigh)) | (Q(last_photo_time__gt=timeLow) & Q(last_photo_time__lt=timeHigh))).filter(product_id=2))\n\n\t\tfor strand in strandsCache:\n\t\t\tphotosByStrandId[strand.id] = list(strand.photos.all())\n\t\t\tusersByStrandId[strand.id] = list(strand.users.all())\n\t\t\tallUsers.extend(strand.users.all())\n\n\t\t\tif len(strand.users.all()) == 0 or len(strand.photos.all()) == 0:\n\t\t\t\tstrandsCache = dealWithDeadStrand(strand, strandsCache)\n\n\t\tallUsers = set(allUsers)\n\n\t\tc = datetime.datetime.now()\n\t\tlogger.debug(\"Building Strands cache with %s strands took took %s milli\" % (len(strandsCache), ((c-b).microseconds/1000) + (c-b).seconds*1000))\n\n\t\tfor photo in photos:\n\t\t\tmatchingStrands = list()\n\n\t\t\tfor strand in strandsCache:\t\t\n\t\t\t\tif strands_util.photoBelongsInStrand(photo, strand, photosByStrandId):\n\t\t\t\t\tmatchingStrands.append(strand)\n\t\t\t\n\t\t\tif len(matchingStrands) == 1:\n\t\t\t\tstrand = matchingStrands[0]\n\t\t\t\tif strands_util.addPhotoToStrand(strand, photo, photosByStrandId, usersByStrandId, strandPhotosToCreate, strandUsersToCreate):\n\t\t\t\t\tstrandsAddedTo.append(strand)\n\t\t\t\t\tphotoToStrandIdDict[photo] = strand.id\n\t\t\t\t\t\n\t\t\t\t\tlogger.debug(\"Just added photo %s to strand %s users %s\" % (photo.id, strand.id, usersByStrandId[strand.id]))\n\t\t\telif len(matchingStrands) > 1:\n\t\t\t\tlogger.debug(\"Found %s matching strands for photo %s, merging\" % (len(matchingStrands), photo.id))\n\t\t\t\ttargetStrand = matchingStrands[0]\n\t\t\t\t\n\t\t\t\t# Merge strands\n\t\t\t\tfor i, strand in enumerate(matchingStrands):\n\t\t\t\t\tif i > 0:\n\t\t\t\t\t\tstrands_util.mergeStrands(targetStrand, strand, photosByStrandId, usersByStrandId, strandPhotosToCreate, strandUsersToCreate)\n\t\t\t\t\t\tlogger.debug(\"Merged strand %s into %s users %s\" % (strand.id, targetStrand.id, usersByStrandId[targetStrand.id]))\n\n\t\t\t\t# Delete unneeded Srands\n\t\t\t\tfor i, strand in enumerate(matchingStrands):\n\t\t\t\t\tif i > 0:\n\t\t\t\t\t\t# remove from our cache and db\n\t\t\t\t\t\tstrandsCache = filter(lambda a: a.id != strand.id, strandsCache)\n\t\t\t\t\t\tlogger.debug(\"Deleted strand %s\" % strand.id)\n\t\t\t\t\t\tstrand.delete()\n\t\t\t\t\t\tstrandsDeleted += 1\n\t\t\t\t\n\t\t\t\tstrandsAddedTo.append(targetStrand)\n\t\t\t\tphotoToStrandIdDict[photo] = targetStrand.id\n\t\t\telse:\n\t\t\t\tnewStrand = Strand.objects.create(user = user, first_photo_time = photo.time_taken, last_photo_time = photo.time_taken, location_point = photo.location_point, location_city = photo.location_city, private = True)\n\t\t\t\tnewStrand.save()\n\t\t\t\t\n\t\t\t\tif strands_util.addPhotoToStrand(newStrand, photo, photosByStrandId, usersByStrandId, strandPhotosToCreate, strandUsersToCreate):\n\t\t\t\t\tstrandsCreated.append(newStrand)\n\t\t\t\t\tstrandsCache.append(newStrand)\n\n\t\t\t\t\tphotoToStrandIdDict[photo] = newStrand.id\n\n\t\t\t\t\tlogger.debug(\"Created new private Strand %s for photo %s and user %s\" % (newStrand.id, photo.id, usersByStrandId[newStrand.id]))\n\t\n\t\t\tphoto.strand_evaluated = True\n\t\t\n\t\tPhoto.bulkUpdate(photos, [\"strand_evaluated\", \"is_dup\"])\n\t\tif len(strandPhotosToCreate) > 0:\n\t\t\ttry:\n\t\t\t\tStrand.photos.through.objects.bulk_create(strandPhotosToCreate)\n\t\t\texcept IntegrityError:\n\t\t\t\tfor obj in strandUsersToCreate:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj.save()\n\t\t\t\t\texcept IntegrityError:\n\t\t\t\t\t\tlogger.error(\"Got IntegrityError trying to save photo %s and strand %s\" % (obj.photo_id, obj.strand_id))\n\n\t\tif len(strandUsersToCreate) > 0:\n\t\t\ttry:\n\t\t\t\tStrand.users.through.objects.bulk_create(strandUsersToCreate)\n\t\t\texcept IntegrityError:\n\t\t\t\tfor obj in strandUsersToCreate:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj.save()\n\t\t\t\t\texcept IntegrityError:\n\t\t\t\t\t\tlogger.error(\"Got IntegrityError trying to save user %s and strand %s\" % (obj.user_id, obj.strand_id))\n\n\n\t\tstrandsToUpdate = list()\n\t\tfor strand in strandsAddedTo:\n\t\t\tif not strand.cache_dirty:\n\t\t\t\tstrand.cache_dirty = True\n\t\t\t\tstrandsToUpdate.append(strand)\n\n\t\tStrand.bulkUpdate(strandsToUpdate, ['cache_dirty'])\n\t\t\n\t\t# We're doing this here instead of when the object was created\n\t\t# so we make sure everything is processed at a minimum level before neighboring starts\n\t\t# Note: now that we're doing async, this could go above if that is needed\n\t\tfor strand in strandsCreated:\n\t\t\tstrand.neighbor_evaluated = False\n\n\t\t# This looks at all the Location Records to see if one has a hint for location for a strand\n\t\tfillInLocationRecordData(user, strandsCreated)\n\t\t\n\t\tStrand.bulkUpdate(strandsCreated, ['neighbor_evaluated', 'location_point'])\n\n\t\tlogger.debug(\"Created %s new strands and updated %s\" % (len(strandsCreated), len(strandsAddedTo)))\n\t\ttotal += len(strandsCreated)\n\t\ttotal += len(strandsAddedTo)\n\t\t\n\t\t# Theoretically this could include a strand multiple times if another thread set the dirty bit\n\t\tids = Strand.getIds(strandsAddedTo)\n\t\tids.extend(Strand.getIds(strandsCreated))\n\t\tif len(ids) > 0:\n\t\t\tpopcaches.processPrivateStrandIds.delay(ids)\n\n\t\tif len(strandsCreated) > 0:\n\t\t\tneighboring.processStrandIds.delay(Strand.getIds(strandsCreated))\n\t\tif len(strandsAddedTo) > 0:\n\t\t\tsuggestion_notifications.processIds.delay(Strand.getIds(strandsAddedTo))\n\n\t\t#logging.getLogger('django.db.backends').setLevel(logging.ERROR)\n\t\t\n\t\t#logger.debug(\"Starting sending notifications...\")\n\t\t# Turning off notifications since the caching script should do this now.\n\t\t# If not, then turn this back on\n\t\t#sendNotifications(photoToStrandIdDict, usersByStrandId, timeWithinSecondsForNotification)\n\n\t\tlogger.info(\"%s photos evaluated and %s strands created, %s strands added to, %s deleted.  Total run took: %s milli\" % (len(photos), len(strandsCreated), len(strandsAddedTo), strandsDeleted, (((datetime.datetime.now()-a).microseconds/1000) + (datetime.datetime.now()-a).seconds*1000)))\n\n\treturn total\n", "error_marker": [115, 15, 115, 34], "repair": "strandPhotosToCreate"}
{"bug_type": "varmisuse", "project": "dagster", "project_url": "https://github.com/dagster-io/dagster", "commit_sha": "df55cc574ac6c37f6a312ddaf81e6dbd83eecde3", "comodified": true, "input_text": "    def start_schedule(self, instance, repository, schedule_name):\n        schedule = instance.get_schedule_by_name(repository, schedule_name)\n        if not schedule:\n            raise DagsterInvariantViolationError(\n                'You have attempted to start schedule {name}, but it does not exist.'.format(\n                    name=schedule_name\n                )\n            )\n\n        if schedule.status == ScheduleStatus.RUNNING:\n            raise DagsterInvariantViolationError(\n                'You have attempted to start schedule {name}, but it is already running'.format(\n                    name=schedule_name\n                )\n            )\n\n        started_schedule = schedule.with_status(ScheduleStatus.RUNNING)\n        instance.update_schedule(repository, started_schedule)\n        return schedule\n", "error_marker": [18, 15, 18, 23], "repair": "started_schedule"}
{"bug_type": "varmisuse", "project": "kornia", "project_url": "https://github.com/kornia/kornia", "commit_sha": "1f44bee7a141e1876a1589df13d8c8d3ee987598", "comodified": false, "input_text": "def dice_loss(input: torch.Tensor, target: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n    r\"\"\"Criterion that computes S\u00f8rensen-Dice Coefficient loss.\n\n    According to [1], we compute the S\u00f8rensen-Dice Coefficient as follows:\n\n    .. math::\n\n        \\text{Dice}(x, class) = \\frac{2 |X| \\cap |Y|}{|X| + |Y|}\n\n    Where:\n       - :math:`X` expects to be the scores of each class.\n       - :math:`Y` expects to be the one-hot tensor with the class labels.\n\n    the loss, is finally computed as:\n\n    .. math::\n\n        \\text{loss}(x, class) = 1 - \\text{Dice}(x, class)\n\n    Reference:\n        [1] https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n\n    Args:\n        input (torch.Tensor): logits tensor with shape :math:`(N, C, H, W)` where C = number of classes.\n        labels (torch.Tensor): labels tensor with shape :math:`(N, H, W)` where each value\n          is :math:`0 \u2264 targets[i] \u2264 C\u22121`.\n        eps (float, optional): Scalar to enforce numerical stabiliy. Default: 1e-8.\n\n    Return:\n        torch.Tensor: the computed loss.\n\n    Example:\n        >>> N = 5  # num_classes\n        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n        >>> output = dice_loss(input, target)\n        >>> output.backward()\n    \"\"\"\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n                        .format(type(input)))\n\n    if not len(input.shape) == 4:\n        raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n                         .format(input.shape))\n\n    if not input.shape[-2:] == target.shape[-2:]:\n        raise ValueError(\"input and target shapes must be the same. Got: {} and {}\"\n                         .format(input.shape, input.shape))\n\n    if not input.device == target.device:\n        raise ValueError(\n            \"input and target must be in the same device. Got: {} and {}\" .format(\n                input.device, target.device))\n\n    # compute softmax over the classes axis\n    input_soft: torch.Tensor = F.softmax(input, dim=1)\n\n    # create the labels one hot tensor\n    target_one_hot: torch.Tensor = one_hot(\n        target, num_classes=input.shape[1],\n        device=input.device, dtype=input.dtype)\n\n    # compute the actual dice score\n    dims = (1, 2, 3)\n    intersection = torch.sum(input_soft * target_one_hot, dims)\n    cardinality = torch.sum(input_soft + target_one_hot, dims)\n\n    dice_score = 2. * intersection / (cardinality + eps)\n\n    return torch.mean(-dice_score + 1.)\n", "error_marker": [48, 46, 48, 51], "repair": "target"}
{"bug_type": "varmisuse", "project": "MoDevETL", "project_url": "https://github.com/klahnakoski/MoDevETL", "commit_sha": "12162c02d55a6136a5655c5e6f6e0ad0e4328e1d", "comodified": true, "input_text": "def _normalize(esfilter):\n    \"\"\"\n    DO NOT USE Structs, WE ARE SPENDING TOO MUCH TIME WRAPPING/UNWRAPPING\n    REALLY, WE JUST COLLAPSE CASCADING and AND or FILTERS\n    \"\"\"\n    if esfilter is TRUE_FILTER or esfilter is FALSE_FILTER or esfilter.isNormal:\n        return esfilter\n\n    # Log.note(\"from: \" + CNV.object2JSON(esfilter))\n    isDiff = True\n\n    while isDiff:\n        isDiff = False\n\n        if esfilter[\"and\"]:\n            output = []\n            for a in esfilter[\"and\"]:\n                a_ = normalize(a)\n                if a_ is not a:\n                    isDiff = True\n                a = a_\n                if a == TRUE_FILTER:\n                    isDiff = True\n                    continue\n                if a == FALSE_FILTER:\n                    isDiff = True\n                    output = None\n                    break\n                if a.get(\"and\", None):\n                    isDiff = True\n                    a.isNormal = None\n                    output.extend(a.get(\"and\", None))\n                else:\n                    a.isNormal = None\n                    output.append(a)\n            if not output:\n                return TRUE_FILTER\n            elif len(output) == 1:\n                # output[0].isNormal = True\n                esfilter = output[0]\n                break\n            elif isDiff:\n                esfilter[\"and\"] = output\n            continue\n\n        if esfilter[\"or\"]:\n            output = []\n            for a in esfilter[\"or\"]:\n                a = _normalize(a)\n                if a_ is not a:\n                    isDiff = True\n                a = a_\n\n                if a == TRUE_FILTER:\n                    isDiff = True\n                    output = None\n                    break\n                if a == FALSE_FILTER:\n                    isDiff = True\n                    continue\n                if a.get(\"or\", None):\n                    a.isNormal = None\n                    isDiff = True\n                    output.extend(a[\"or\"])\n                else:\n                    a.isNormal = None\n                    output.append(a)\n            if not output:\n                return FALSE_FILTER\n            elif len(output) == 1:\n                esfilter = output[0]\n                break\n            elif isDiff:\n                esfilter[\"or\"] = output\n            continue\n\n    esfilter.isNormal = True\n    return esfilter\n", "error_marker": [48, 16, 48, 17], "repair": "a_"}
{"bug_type": "varmisuse", "project": "trello-card-delay", "project_url": "https://github.com/Kulikjak/trello-card-delay", "commit_sha": "f9021ead066646ec79d2614692a965bf2c27358f", "comodified": false, "input_text": "def _run_trellohelper():\n\n    trello = TrelloApi(TRELLO[\"AppKey\"], TRELLO[\"Token\"])\n\n    # retrieve all cards on the main board\n    data = trello.boards.get_card(TRELLO[\"Board\"], filter=\"all\", checklists=\"all\")\n\n    # we are not interested in cards, which are closed\n    # and without any progress labels\n    progress_labels = {LABEL[\"Snooze\"], LABEL[\"Tomorrow\"]}\n    cards = [c for c in data if not c[\"closed\"] or set(c[\"idLabels\"]) & progress_labels]\n\n    logging.info(f\"Total board/open cards: {len(data)}/{len(cards)}\")\n\n    for card in cards:\n\n        # check whether everything is ok with this card\n        ok, reason = integrity_check(card)\n        if not ok:\n            # broken cards need to be returned\n            trello.cards.update(\n                card[\"id\"],\n                due=\"null\",\n                closed=\"false\",\n                name=f\"[RESTORED] {card['name']}\",\n                desc=f\"**This card was restored**\\n{reason}\\n\\n{card['desc']}\",\n                idLabels=card[\"idLabels\"] + [LABEL[\"Important\"]]\n            )\n            continue\n\n        # handle dollar snooze shortcuts\n        handle_dollar_snoozing(trello, card)\n\n        # snooze cards with date and delay label\n        snooze_card(trello, card)\n\n        # wake me up when time comes\n        wake_card(trello, card)\n\n    # schedule cards for tomorrow\n    current_time = datetime.now().time()\n    if time(1, 0, 0) <= current_time <= time(2, 0, 0):\n        logging.info(f\"[{datetime.now()}]: running tomorrow scheduling\")\n\n        for card in cards:\n            # work with relevant cards only\n            if LABEL[\"Tomorrow\"] not in card[\"idLabels\"]:\n                continue\n\n            card[\"idList\"] = LIST[\"Tomorrow\"]\n            card[\"idLabels\"].remove(LABEL[\"Tomorrow\"])\n\n            trello.cards.update(card[\"id\"], idList=card[\"idList\"])\n            # labels cannot be removed with update in some cases (where none would remain)\n            trello.cards.delete_idLabel_idLabel(LABEL[\"Tomorrow\"], card[\"id\"])\n            logging.info(\n                f\"[{datetime.now()}]: Card scheduled for tomorrow: {card['id']} : {card['name']}\"\n            )\n\n    # handle projects and project related labels\n    cards = [c for c in data if any(a[\"color\"] is None for a in c[\"labels\"])]\n\n    projects = {}\n    project_cards = []\n\n    # at first go through individual cards (not the project one)\n    for card in cards:\n\n        if card[\"idList\"] == LIST[\"Projects\"]:\n            # save project related cards for later quicker use\n            project_cards.append(card)\n            continue\n\n        if LABEL[\"Snooze\"] in card[\"idLabels\"]:\n            # do not consider snoozed cards in any way\n            continue\n\n        for label in card[\"labels\"]:\n\n            if label[\"color\"] is not None:\n                continue\n\n            if label[\"id\"] not in projects:\n                projects[label[\"id\"]] = []\n\n            projects[label[\"id\"]].append((card[\"name\"], card[\"closed\"]))\n\n    # now handle project checklists\n    for card in project_cards:\n\n        for label in card[\"labels\"]:\n            # continue only with colorless labels with the project name\n            if label[\"name\"] != card[\"name\"] or label[\"color\"] is not None:\n                continue\n\n            if label[\"id\"] not in projects:\n                # this label is not present on any other card\n                # and thus the checklist would be empty\n                continue\n\n            # get project related checklist\n            checklist = next(\n                (c for c in card[\"checklists\"] if c[\"name\"] == f\"@{card['name']}\"), None\n            )\n            if checklist is None:\n                # create new checklist if one does not exist yet\n                checklist = trello.cards.new_checklist(card[\"id\"], name=f\"@{card['name']}\")\n\n            for item in checklist[\"checkItems\"]:\n                for desc, closed in projects[label[\"id\"]]:\n                    if desc != item[\"name\"]:\n                        continue\n\n                    if closed and item[\"state\"] != \"complete\":\n                        # change status to complete\n                        trello.cards.update_checkItem_idCheckItem(\n                            item[\"id\"], card[\"id\"], state=\"complete\"\n                        )\n\n                    if not closed and item[\"state\"] != \"incomplete\":\n                        # change status to incomplete\n                        trello.cards.update_checkItem_idCheckItem(\n                            item[\"id\"], card[\"id\"], state=\"incomplete\"\n                        )\n\n                    # remove this from projects\n                    projects[label[\"id\"]].remove((desc, closed))\n                    break\n                else:\n                    trello.checklists.delete_checkItem_idCheckItem(\n                        item[\"id\"], checklist[\"id\"]\n                    )\n\n            for desc, closed in projects[label[\"id\"]]:\n                trello.checklists.new_checkItem(\n                    checklist[\"id\"], desc, checked=\"true\" if closed else \"false\"\n                )\n\n            break\n\n        else:\n            # in case we did not found any label, remove project checklist\n            checklist = next(\n                (c for c in card[\"checklists\"] if c[\"name\"] == f\"@{card['name']}\"), None\n            )\n            if checklist is not None:\n                trello.checklists.delete(checklist[\"id\"])\n\n    # last thing: remove Zapier script checking card\n    for card in cards:\n        if card[\"name\"] == \"[CHECK] Problem with Trello script\":\n            trello.cards.delete(card[\"id\"])\n            logging.info(f\"[{datetime.now()}]: Deleted Zapier check card\")\n            break\n\n    logging.info(f\"[{datetime.now()}]: Done\")\n", "error_marker": [149, 16, 149, 21], "repair": "data"}
{"bug_type": "varmisuse", "project": "machine-learning-python", "project_url": "https://github.com/DoneHome/machine-learning-python", "commit_sha": "e6b8b52375e2b4f04be45e203626b2e5360d1448", "comodified": true, "input_text": "    def select_alpha_J(self, i):\n        \"\"\"\n        \"\"\"\n        E_i = self.E_val[i]\n\n        max_diff = 0\n        j = -1\n        E_j = 0\n\n        for k in range(self.num_train):\n            if k == i:\n                continue\n            E_k = self.calc_decision_error(k)\n            diff = abs(E_i - E_k)\n            if diff > max_diff:\n                max_diff = diff\n                j = k\n                E_j = E_k\n\n        self.E_val[j] = E_k\n        return j\n", "error_marker": [19, 24, 19, 27], "repair": "E_j"}
{"bug_type": "varmisuse", "project": "neuralyzer", "project_url": "https://github.com/michigraber/neuralyzer", "commit_sha": "57a2a5335d830838b99fed3370e4f9ead2b44d43", "comodified": false, "input_text": "def get_meta_file(path, meta_file_name='data_meta.json', dirretracts=3):\n    ''' Search for data_meta.json file in the parent directories '''\n    if os.path.isfile(trunk):\n        trunk, _ = os.path.split(path)\n    else:\n        trunk = path\n    for _ in range(dirretracts):\n        meta_file = os.path.join(trunk, meta_file_name)\n        if os.path.exists(meta_file):\n            return meta_file\n        trunk, tail = os.path.split(trunk)\n", "error_marker": [2, 22, 2, 27], "repair": "path"}
{"bug_type": "varmisuse", "project": "simtrans", "project_url": "https://github.com/fkanehiro/simtrans", "commit_sha": "0d08309cf19f7c716870b55771b7b567480b939b", "comodified": false, "input_text": "    def write(self, mdata, fname, options=None):\n        '''\n        Write simulation model in VRML format\n        '''\n        self._options = options\n        fpath, fext = os.path.splitext(fname)\n        basename = os.path.basename(fpath)\n        dirname = os.path.dirname(fname)\n        if mdata.name is None or mdata.name == '':\n            mdata.name = basename\n\n        # convert revolute2 joint to two revolute joints (with a link\n        # in between)\n        for j in mdata.joints:\n            if j.jointType == model.JointModel.J_REVOLUTE2:\n                logging.info(\"converting revolute2 joint to two revolute joints\")\n                nl = model.LinkModel()\n                nl.name = j.name + \"_REVOLUTE2_LINK\"\n                nl.matrix = j.getmatrix()\n                nl.trans = None\n                nl.rot = None\n                nl.mass = 0.001 # assign very small mass\n                mdata.links.append(nl)\n                nj = copy.deepcopy(j)\n                nj.name = j.name + \"_SECOND\"\n                nj.jointType = model.JointModel.J_REVOLUTE\n                nj.parent = nl.name\n                nj.child = j.child\n                nj.axis = j.axis2\n                mdata.joints.append(nj)\n                j.jointType = model.JointModel.J_REVOLUTE\n                j.child = nl.name\n\n        # check for same names in visuals or collisions\n        usednames = {}\n        for l in mdata.links:\n            for v in l.visuals:\n                if v.name in usednames:\n                    v.name = l.name + \"-visual\"\n                    if v.name in usednames:\n                        v.name = l.name + \"-visual-\" + str(uuid.uuid1()).replace('-', '')\n                usednames[v.name] = True\n            for c in l.collisions:\n                if c.name in usednames:\n                    c.name = l.name + \"-collision\"\n                    if c.name in usednames:\n                        c.name = l.name + \"-collision-\" + str(uuid.uuid1()).replace('-', '')\n                usednames[c.name] = True\n\n        # find root joint (including local peaks)\n        self._roots = utils.findroot(mdata)\n\n        # render the data structure using template\n        loader = jinja2.PackageLoader(self.__module__, 'template')\n        env = jinja2.Environment(loader=loader, extensions=['jinja2.ext.do'])\n\n        self._linkmap['world'] = model.LinkModel()\n        for m in mdata.links:\n            self._linkmap[m.name] = m\n\n        # render shape vrml file for each links\n        shapefilemap = {}\n        for l in mdata.links:\n            shapes = copy.copy(l.visuals)\n            if options.usecollision:\n                shapes = copy.copy(l.collisions)\n            if options.useboth:\n                shapes.extend(copy.copy(l.collisions))\n            for v in shapes:\n                logging.info('writing shape of link: %s, type: %s' % (l.name, v.shapeType))\n                if v.shapeType == model.ShapeModel.SP_MESH:\n                    template = env.get_template('vrml-mesh.wrl')\n                    if isinstance(v.data, model.MeshTransformData):\n                        v.data.pretranslate()\n                    m = {}\n                    m['children'] = [v.data]\n                    shapefname = (mdata.name + \"-\" + l.name + \"-\" + v.name + \".wrl\").replace('::', '_')\n                    with open(os.path.join(dirname, shapefname), 'w') as ofile:\n                        ofile.write(template.render({\n                            'name': v.name,\n                            'ShapeModel': model.ShapeModel,\n                            'mesh': m\n                        }))\n                    shapefilemap[v.name] = shapefname\n\n        # render main vrml file for each bodies\n        template = env.get_template('vrml.wrl')\n        roots = []\n        modelfiles = {}\n        for root in self._roots:\n            if root == 'world':\n                for r in utils.findchildren(m, root):\n                    roots.append((r.child, \"fixed\"))\n            else:\n                roots.append((root, \"free\"))\n        for r in roots:\n            logging.info('writing model for %s' % r[0])\n            if len(roots) == 1:\n                mfname = fname\n            else:\n                mfname = (mdata.name + \"-\" + r.child + \".wrl\").replace('::', '_')\n            self.renderchildren(mdata, r[0], r[1], os.path.join(dirname, mfname), shapefilemap, template)\n            modelfiles[mfname] = self._linkmap[r[0]]\n        \n        # render openhrp project\n        template = env.get_template('openhrp-project.xml')\n        with open(fname.replace('.wrl', '-project.xml'), 'w') as ofile:\n            ofile.write(template.render({\n                'models': modelfiles,\n            }))\n\n        # render choreonoid project\n        template = env.get_template('choreonoid-project.yaml')\n        with open(fname.replace('.wrl', '-project.cnoid'), 'w') as ofile:\n            ofile.write(template.render({\n                'models': modelfiles,\n            }))\n", "error_marker": [91, 44, 91, 45], "repair": "mdata"}
{"bug_type": "varmisuse", "project": "vSPC.py", "project_url": "https://github.com/mbattersby/vSPC.py", "commit_sha": "b5a8f088336a7c384da38e080404e6cce40fec49", "comodified": false, "input_text": "    def _setup_poll(self, fd):\n        mask = 0\n        if fd in self.read_handlers:\n            mask = mask | select.POLLIN\n        if fd in self.write_handlers:\n            mask = mask | select.POLLOUT\n        if fd:\n            self.poller.register(fd, mask)\n        else:\n            try:\n                self.poller.unregister(fd)\n            except KeyError:\n                pass\n", "error_marker": [6, 11, 6, 13], "repair": "mask"}
{"bug_type": "varmisuse", "project": "neuropype_ephy", "project_url": "https://github.com/annapasca/neuropype_ephy", "commit_sha": "18b49e44b001feb58f09f8ef3360fe3974f1bf1c", "comodified": false, "input_text": "    def _run_interface(self, runtime):\n\n        sbj_id = self.inputs.sbj_id\n        sbj_dir = self.inputs.sbj_dir\n        raw_info = self.inputs.raw_info\n        raw_fname = self.inputs.raw_fname\n        aseg = self.inputs.aseg\n        is_blind = self.inputs.is_blind\n        spacing = self.inputs.spacing\n        aseg_labels = self.inputs.aseg_labels\n\n        self.fwd_filename = self._get_fwd_filename(raw_fname, aseg,\n                                                   spacing, is_blind)\n\n        # check if we have just created the fwd matrix\n        if not op.isfile(self.fwd_filename):\n            bem = create_bem_sol(sbj_dir, sbj_id)  # bem solution\n\n            src = create_src_space(sbj_dir, sbj_id, spacing, is_blind)  # src space\n\n            if aseg:\n                src = create_mixed_source_space(sbj_dir, sbj_id, spacing,\n                                                aseg_labels, src)\n\n            n = sum(src[i]['nuse'] for i in range(len(src)))\n            print('il src space contiene %d spaces e %d vertici'\n                  % (len(src), n))\n\n            trans_fname = is_trans(raw_info)\n\n            # TODO: ha senso una funzione con un solo cmd?\n            compute_fwd_sol(raw_info, trans_fname, src, bem, self.fwd_filename)\n        else:\n            print '\\n*** FWD file %s exists!!!\\n' % self.fwd_filename\n\n        return runtime\n", "error_marker": [28, 35, 28, 43], "repair": "raw_fname"}
